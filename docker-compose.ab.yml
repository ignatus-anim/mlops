services:
  infer-prod:
    build:
      context: ./inference/backend
    env_file:
      - .env  # load AWS and other shared secrets
    environment:
      - MODEL_PREFIX=best_model
      - MLFLOW_S3_BUCKET=${MLFLOW_S3_BUCKET:-mlops-bucket0982}
    ports:
      - "6001:6001"

  infer-cand:
    build:
      context: ./inference/backend
    env_file:
      - .env
    environment:
      - MODEL_PREFIX=candidate_model
      - MLFLOW_S3_BUCKET=${MLFLOW_S3_BUCKET:-mlops-bucket0982}
    # expose candidate backend on host port 6002 -> container port 6001
    ports:
      - "6002:6001"

  router:
    image: nginx:1.27-alpine
    volumes:
      - ./router/nginx.conf:/etc/nginx/nginx.conf:ro
    ports:
      - "7000:7000"
    depends_on:
      - infer-prod
      - infer-cand
